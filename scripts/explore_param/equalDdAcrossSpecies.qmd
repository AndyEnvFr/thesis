---
title: ""
format: html
editor: visual
---

```{r}
#| warning: false
source("~/cyber_synch/git_synch/scripts/functions.R")
library(PhyloSim)
library(parallel)
library(lattice)
# source("~/Uni/Master/MA/git_synch/scripts/functions.R")
```

## skip to --------------------------------------------- and load results

```{r}
#| eval: true
#| echo: false

# Define parameter values
density_options <- c(1)
dispersal_options <- list(1)  # Using a list to preserve types
densityCut_options <- c(1)
fitnessBaseMortalityRatio_options <- c(1,2,50)

# Initialize empty list for parameters
params <- list()
param_index <- 1

# Generate all parameter combinations
for (density in density_options) {
  for (dispersal_type in dispersal_options) {
    for (cut in densityCut_options) {
      for (fbmr in fitnessBaseMortalityRatio_options) {
        
        # Format scenario components
        scenario_density <- gsub("\\.", "-", as.character(density))
        scenario_dispersal <- ifelse(identical(dispersal_type, "global"), "G", dispersal_type)
        scenario_cut <- gsub("\\.", "-", as.character(cut))
        scenario_fbmr <- gsub("\\.", "-", as.character(fbmr))
        
        # Create parameter set
        params[[param_index]] <- createCompletePar(
          x = 50,
          y = 50,
          density = density,
          dispersal = dispersal_type,
          specRate = 2,
          environment = 0,
          fitnessBaseMortalityRatio = fbmr,
          densityCut = cut,
          seed = 20250414,
          type = "base",
          protracted = 0,
          fission = 0,
          redQueen = 0,
          redQueenStrength = 0,
          airmat = 0,
          fitnessActsOn = "mortality",
          runs = c(sort( c( ((1000:1050 * 50)), ((1000:1050 * 50)) + 1 ))),
          scenario = paste0("dd", scenario_density,
                          "_disp", scenario_dispersal,
                          "_sr2_e0_m", scenario_fbmr,
                          "_cut", scenario_cut)
        )
        
        # Increment parameter index
        param_index <- param_index + 1
      }
    }
  }
}
```

```{r}
# runs <- runSimulationBatch(params, parallel = 3)
```

```{r}
names(runs) <- run_name(runs = runs, batch = TRUE)
# saveRDS(runs, "~/cyber_synch/local/runs/test/equalDdAcrossSpecies")
# print(names(runs))
```


## --------------------------------------------- load results
```{r}
runs <- readRDS("~/cyber_synch/local/runs/test/equalDdAcrossSpecies")
```

# convert into tabular
```{r}
runz <- torus_batch(runs, overwrite = TRUE, max_neighborhood_radius = NULL)

cl <- makeCluster(length(runz)) # make cluster with core nb
clusterExport(cl, c("runz", "get_con_neigh", "get_circular_offsets"))
parallel_function <- function(run){
  result <- get_con_neigh(run = run, radius = run$Model$densityCut, undo_torus = TRUE)
  return(result)
}
runz <- parLapply(cl, X = runz, fun = parallel_function)
```

```{r}
cl <- makeCluster(length(runz))
clusterExport(cl, c("mat_to_tab", "runz"))
res <- parLapply(cl = cl, X = runz, fun = mat_to_tab)
```


# statistical analysis
```{r}
tab_list <- lapply(res, function(x){
  return(x %>% mutate(census = as.numeric(census)))
})
```
## for all species aggregated
### con N 
```{r}
# glm for all species aggregated

fm1 <- glm(formula = "mort ~ con", family = binomial(), data = tab_list[[1]]) # no dd
fm2 <- glm(formula = "mort ~ con", family = binomial(), data = tab_list[[3]]) # dd

summary(fm1)
cat("\n\n")
summary(fm2)
```

### mort change
```{r}
# glm for all species aggregated

# without DD
fm1 <- glm(formula = "mort ~ con", family = binomial(), data = tab_list[[1]]) # no dd
sfm1 <- summary(fm1)$coefficients
mort0 <- plogis(sfm1["(Intercept)", "Estimate"])
mort1 <- plogis(sfm1["con", "Estimate"] * 1 + sfm1["(Intercept)", "Estimate"])
change_mort <- (mort1 - mort0)
cat("change_mort for run without DD: ", change_mort)

# with DD
fm_dd <- glm(formula = "mort ~ con", family = binomial(), data = tab_list[[3]]) # dd
sfm_dd <- summary(fm_dd)$coefficients
mort0_dd <- plogis(sfm_dd["(Intercept)", "Estimate"])
mort1_dd <- plogis(sfm_dd["con", "Estimate"] * 1 + sfm_dd["(Intercept)", "Estimate"])
change_mort_dd <- (mort1_dd - mort0_dd)
cat("\n\nchange_mort for run with DD: ", change_mort_dd)
```









## species wise model, if spec has >= 10 individuals
### without DD
```{r}
# for every species do a run
fbmr1 <- tab_list$ddT1_disp1_sr2_eF_fbmr1_dc1
spec_id <- unique(fbmr1$spec_id)
length(spec_id)
# table(fbmr1$spec_id)

# tale only species with at lease 10 individuals
fbmr1_ <- fbmr1 %>%
  group_by(spec_id) %>%
  mutate(freq_spec = n()) %>% 
  filter(freq_spec >= 50)
```

```{r}
glm_result <- data.frame("con_coef" = NA, "spec_id" = NA, "spec_N" = NA, "change_mort" = NA)
run <- 1

for (id in unique(fbmr1_$spec_id)) {
  df <- fbmr1_ %>% filter(spec_id == id)
  fm <- glm(formula = "mort ~ con", family = binomial(), data = df)
  sfm <- summary(fm)$coefficients
  
  mort0 <- plogis(sfm["(Intercept)", "Estimate"])
  mort1 <- plogis(sfm["con", "Estimate"] * 1 + sfm["(Intercept)", "Estimate"])
  change_mort <- (mort1 - mort0)
  
  glm_result[run, ] <- c(sfm["con", "Estimate"], id, unique(df$freq_spec), change_mort)
  
  run <- run + 1
}
```

```{r}
# filter out outlier
# glm_result <- glm_result %>%  filter(con_coef > -10,
#                                      con_coef < 10)
```

```{r}
#visualize


fm1 <- lm(con_coef ~ log(spec_N), data = glm_result) %>% summary() %>% coef()
plot(con_coef ~ log(spec_N), data = glm_result)
abline(fm1["(Intercept)", "Estimate"], fm1["log(spec_N)", "Estimate"])
abline(h = 0, lty = 2)

fm2 <- lm(change_mort ~ log(spec_N), data = glm_result) %>% summary() %>% coef()
plot(change_mort ~ log(spec_N), data = glm_result)
abline(fm2["(Intercept)", "Estimate"], fm2["log(spec_N)", "Estimate"])
abline(h = 0, lty = 2)

cat("con_coef\n")
print(fm1)
cat("\n\n")
cat("mort_change\n")
print(fm2)

cat(paste0("\n\ncon_coef median ",median(glm_result$con_coef)))
cat(paste0("\n\ncon_coef mean ",mean(glm_result$con_coef)))
cat(paste0("\n\nchange_mort median ",median(glm_result$change_mort)))
```




### with DD
```{r}
# for every species do a run
fbmr50 <- tab_list$ddT1_disp1_sr2_eF_fbmr50_dc1 # with DD
spec_id <- unique(fbmr50$spec_id)
length(spec_id)
# table(fbmr50$spec_id)

# tale only species with at lease 10 individuals
fbmr50_ <- fbmr50 %>%
  group_by(spec_id) %>%
  mutate(freq_spec = n()) %>% 
  filter(freq_spec >= 50)
```

```{r}
glm_result <- data.frame("con_coef" = NA, "spec_id" = NA, "spec_N" = NA, "change_mort" = NA)
run <- 1

for (id in unique(fbmr50_$spec_id)) {
  df <- fbmr50_ %>% filter(spec_id == id)
  fm <- glm(formula = "mort ~ con", family = binomial(), data = df)
  sfm <- summary(fm)$coefficients
  
  mort0 <- plogis(sfm["(Intercept)", "Estimate"])
  mort1 <- plogis(sfm["con", "Estimate"] * 1 + sfm["(Intercept)", "Estimate"])
  change_mort <- (mort1 - mort0)
  
  glm_result[run, ] <- c(sfm["con", "Estimate"], id, unique(df$freq_spec), change_mort)
  
  run <- run + 1
}
```

```{r}
# filter out outlier
# glm_result <- glm_result %>%  filter(con_coef > -10,
#                                      con_coef < 10)
```

```{r}
#visualize

fm1 <- lm(con_coef ~ log(spec_N), data = glm_result) %>% summary() %>% coef()
plot(con_coef ~ log(spec_N), data = glm_result)
abline(fm1["(Intercept)", "Estimate"], fm1["log(spec_N)", "Estimate"])
abline(h = 0, lty = 2)

fm2 <- lm(change_mort ~ log(spec_N), data = glm_result) %>% summary() %>% coef()
plot(change_mort ~ log(spec_N), data = glm_result)
abline(fm2["(Intercept)", "Estimate"], fm2["log(spec_N)", "Estimate"])
abline(h = 0, lty = 2)

cat("con_coef\n")
print(fm1)
cat("\n\n")
cat("mort_change\n")
print(fm2)

cat(paste0("\n\ncon_coef median ",median(glm_result$con_coef)))
cat(paste0("\n\ncon_coef mean ",mean(glm_result$con_coef)))
cat(paste0("\n\nchange_mort median ",median(glm_result$change_mort)))
```



If we change the filter conditions from >=10 >=50 we get a different trend:
for >=10 the median con_estimates for species wise models is higher without DD
for >=50 the median con_estimates for species wise models is higher with DD (as I would expect)

Because the results seem unstable, Lisa suggested to:
Ich fürchte du solltest doch eine meta-analyse regression machen. das ist so auch in dem Tutorial erklärt. Dann wird die Unsicherheit der estimates mit berücksichtigt




